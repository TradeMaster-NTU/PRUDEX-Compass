{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"..\")\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "data_dir=\"./sz50/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best model is  en_3_batch_256_plr_0.0007_qlr_0.0007_layer_128_2_buffer_10000_discount_0.99_tem_20_bm_0.5_uncertain_0.5\n",
      "the seed_51234 's best train result in the valid is 7\n",
      "the seed_45123 's best train result in the valid is 4\n",
      "the seed_34512 's best train result in the valid is 2\n",
      "the seed_23451 's best train result in the valid is 2\n",
      "the seed_12345 's best train result in the valid is 8\n",
      "the best sr result is  0.42422131268289043\n",
      "the best tr result is  13.764689920784907 %\n"
     ]
    }
   ],
   "source": [
    "model_list=os.listdir(\"/home/sunshuo/qml/RL_Mix/PM/data/portfolio_management/sunrise-pm/sz50\")\n",
    "# the all_file_list are the models we need\n",
    "#here we use sr as our test score and do the trick\n",
    "test_sr_average=[]# this is the list to store every models' average result \n",
    "for i in range(len(model_list)):\n",
    "    result_sr=[]# this is the list to store single models result  \n",
    "    model_path=os.path.join(data_dir,model_list[i])# find one model \n",
    "    seed_file_list=os.listdir(model_path)# find its seed \n",
    "    for seed in seed_file_list:\n",
    "        result=pd.read_csv( os.path.join(data_dir,model_list[i],seed,\"result.csv\"),index_col=0)\n",
    "        num_epoch=list(result[\"eval_sr\"]).index(max(result[\"eval_sr\"])) # find the num_epoch in that random seed to stop and see\n",
    "        seed_best_result=result.iloc[num_epoch][\"test_tr\"]\n",
    "        result_sr.append(seed_best_result)\n",
    "    average_result=np.average(result_sr)\n",
    "    test_sr_average.append(average_result)\n",
    "    # print(test_sr_average,model_list[i] )\n",
    "\n",
    "loc=test_sr_average.index(max(test_sr_average))\n",
    "print(\"the best model is \",model_list[loc])\n",
    "model_path=os.path.join(data_dir,model_list[loc])\n",
    "seed_file_list=os.listdir(model_path)\n",
    "best_results=[]\n",
    "best_crs=[]\n",
    "returns_list=[]\n",
    "for seed in seed_file_list:\n",
    "    result=pd.read_csv( os.path.join(data_dir,model_list[loc],seed,\"result.csv\"),index_col=0)\n",
    "    num_epoch=list(result[\"eval_sr\"]).index(max(result[\"eval_sr\"]))\n",
    "    print(\"the\",seed,\"'s best train result in the valid is\",num_epoch)\n",
    "    returns=pd.read_csv( os.path.join(data_dir,model_list[loc],seed,\"test_daily_return_{}.csv\".format(num_epoch)),index_col=0)\n",
    "    returns=np.array(returns[\"daily_return\"]).tolist()\n",
    "    returns_list.append(returns)\n",
    "    seed_best_result=result.iloc[num_epoch][\"test_sr\"]\n",
    "    seed_best_cr=result.iloc[num_epoch][\"test_tr\"]\n",
    "    best_results.append(seed_best_result)\n",
    "    best_crs.append(seed_best_cr)\n",
    "average_best_result=np.mean(best_results)\n",
    "average_cr=np.mean(best_crs)\n",
    "print(\"the best sr result is \",average_best_result)   \n",
    "print(\"the best tr result is \",average_cr*100,\"%\")   \n",
    "np.save(\"Mixsz50.npy\",np.array(returns_list)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the best baseline is from sac and  cr:377.76Â±982.90    sr:3.03Â±0.87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=os.path.join(data_dir,model_list[loc])\n",
    "seed_file_list=os.listdir(model_path)\n",
    "results=[]\n",
    "for seed in seed_file_list:\n",
    "    result=pd.read_csv( os.path.join(data_dir,model_list[loc],seed,\"result.csv\"),index_col=0)\n",
    "    num_epoch=list(result[\"eval_sr\"]).index(max(result[\"eval_sr\"]))\n",
    "    result=pd.read_csv(os.path.join(data_dir,model_list[loc],seed,\"test_daily_return_{}.csv\".format(num_epoch)),index_col=0)\n",
    "    results.append(result)\n",
    "for result in results:\n",
    "    result[\"reward\"]=result.daily_return+1\n",
    "    y_list=[1]\n",
    "    for x in list(result.reward):\n",
    "        y=y_list[-1]*x\n",
    "        y_list.append(y)\n",
    "    y_list=y_list[1:]\n",
    "    result[\"Cumlative Return\"]=y_list\n",
    "result_1,result_2,result_3,result_4,result_5=results\n",
    "average_df=pd.DataFrame()\n",
    "average_df[\"daily_return\"]=(result_1[\"daily_return\"]+result_2[\"daily_return\"]+result_3[\"daily_return\"]+result_4[\"daily_return\"]+result_5[\"daily_return\"])/5\n",
    "average_df[\"reward\"]=average_df[\"daily_return\"]+1\n",
    "y_list=[1]\n",
    "for x in list(average_df.reward):\n",
    "    y=y_list[-1]*x\n",
    "    y_list.append(y)\n",
    "y_list=y_list[1:]\n",
    "average_df[\"Cumlative Return\"]=y_list\n",
    "average_df[\"result1\"]=result_1[\"Cumlative Return\"]\n",
    "average_df[\"result2\"]=result_2[\"Cumlative Return\"]\n",
    "average_df[\"result3\"]=result_3[\"Cumlative Return\"]\n",
    "average_df[\"result4\"]=result_4[\"Cumlative Return\"]\n",
    "average_df[\"result5\"]=result_5[\"Cumlative Return\"]\n",
    "average_df[\"std\"]=np.std([average_df.result1,average_df.result2,average_df.result3,average_df.result4,average_df.result5],axis=0)\n",
    "average_df.to_csv(\"/home/sunshuo/qml/RL_Mix/sz50exen.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sr_average:0.7563256171452735,sr_std:0.12137778908342486\n",
      "cr_average:0.5212244753607925,cr_std:0.06190739537013845\n",
      "sor_average:0.8349855284715122,sor_std:0.13545166211405132\n",
      "mdd_average:0.2817396289250719,mdd_std:0.019066738343010252\n",
      "vol_average:0.006897265715318709,vol_std:0.00021098673690743555\n"
     ]
    }
   ],
   "source": [
    "def evaualte(df):\n",
    "    daily_return=df[\"daily_return\"]\n",
    "    neg_ret_lst=df[df[\"daily_return\"]<0][\"daily_return\"]\n",
    "    sharpe_ratio=np.mean(daily_return)/np.std(daily_return)*(len(df)**0.5)\n",
    "    vol=np.std(daily_return)\n",
    "    mdd=max((max(df[\"Cumlative Return\"])-df[\"Cumlative Return\"])/max(df[\"Cumlative Return\"]))\n",
    "    cr=np.sum(daily_return)/mdd\n",
    "    sor=np.sum(daily_return)/np.std(neg_ret_lst)/np.sqrt(len(daily_return))\n",
    "    return sharpe_ratio,vol,mdd,cr,sor\n",
    "def all(results):\n",
    "    sharpe_ratios=[]\n",
    "    vols=[]\n",
    "    mdds=[]\n",
    "    crs=[]\n",
    "    sors=[]\n",
    "    for result in results:\n",
    "        sr,vo,mdd,cr,sor=evaualte(result)\n",
    "        sharpe_ratios.append(sr)\n",
    "        vols.append(vo)\n",
    "        mdds.append(mdd)\n",
    "        crs.append(cr)\n",
    "        sors.append(sor)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    print(\"sr_average:{},sr_std:{}\".format(np.mean(sharpe_ratios),np.std(sharpe_ratios)))\n",
    "    print(\"cr_average:{},cr_std:{}\".format(np.mean(crs),np.std(crs)))\n",
    "    print(\"sor_average:{},sor_std:{}\".format(np.mean(sors),np.std(sors) ))\n",
    "    print(\"mdd_average:{},mdd_std:{}\".format(np.mean(mdds),np.std(mdds) ))\n",
    "    print(\"vol_average:{},vol_std:{}\".format(np.mean(vols),np.std(vols) ))\n",
    "all(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../../rl_env/portfolio_management.py:303: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_indicator[\"zclose\"]=(temp_indicator.close/(temp_indicator.close.rolling(2).sum()-temp_indicator.close))-1\n",
      "../../../rl_env/portfolio_management.py:304: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_indicator[\"zd_5\"]=(temp_indicator.adjcp.rolling(5).sum()/5)/temp_indicator.adjcp-1\n",
      "../../../rl_env/portfolio_management.py:305: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_indicator[\"zd_10\"]=(temp_indicator.adjcp.rolling(10).sum()/10)/temp_indicator.adjcp-1\n",
      "../../../rl_env/portfolio_management.py:306: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_indicator[\"zd_15\"]=(temp_indicator.adjcp.rolling(15).sum()/15)/temp_indicator.adjcp-1\n",
      "../../../rl_env/portfolio_management.py:307: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_indicator[\"zd_20\"]=(temp_indicator.adjcp.rolling(20).sum()/20)/temp_indicator.adjcp-1\n",
      "../../../rl_env/portfolio_management.py:308: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_indicator[\"zd_25\"]=(temp_indicator.adjcp.rolling(25).sum()/25)/temp_indicator.adjcp-1\n",
      "../../../rl_env/portfolio_management.py:309: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_indicator[\"zd_30\"]=(temp_indicator.adjcp.rolling(30).sum()/30)/temp_indicator.adjcp-1\n",
      "/home/sunshuo/miniconda3/envs/FinRL/lib/python3.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy mean 3.1273227\n",
      "entropy std 0.027535968\n",
      "ENB mean 1.025773809044568\n",
      "ENB std 0.008091870664550267\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../..\")\n",
    "from rl_env.portfolio_management import get_dataset_config\n",
    "train_config, valid_config, test_config=get_dataset_config(\"sz50\")\n",
    "test=test_config[\"df\"]\n",
    "def sl_targetmaking(df):\n",
    "    \"\"\"making target, aka the future profit margin\"\"\"\n",
    "    df_new=df.sort_values(by=[\"tic\", \"date\"])\n",
    "    unique_ticker = df_new.tic.unique()\n",
    "    df_indicator=pd.DataFrame()\n",
    "    for i in range(len(unique_ticker)):\n",
    "        temp_indicator = df_new[df_new.tic == unique_ticker[i]]\n",
    "        target=temp_indicator.zclose[1:]\n",
    "        target=np.append(target,np.nan)\n",
    "        temp_indicator[\"target\"]=target\n",
    "        df_indicator=df_indicator.append(temp_indicator,ignore_index=True)\n",
    "        \n",
    "    df_indicator = df_indicator.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "    df_indicator.index = df_indicator.date.factorize()[0]\n",
    "    return df_indicator\n",
    "new_test=sl_targetmaking(test)\n",
    "def softmax_normalization(actions):\n",
    "    numerator = np.exp(actions)\n",
    "    denominator = np.sum(np.exp(actions))\n",
    "    softmax_output = numerator/denominator\n",
    "    return softmax_output\n",
    "def log(action):\n",
    "    action=np.log(action)\n",
    "    return action\n",
    "def get_kde(x,data_array,bandwidth):\n",
    "    def gauss(x):\n",
    "        import math\n",
    "        return (1/math.sqrt(2*math.pi))*math.exp(-0.5*(x**2))\n",
    "    N=len(data_array)\n",
    "    res=0\n",
    "    if len(data_array)==0:\n",
    "        return 0\n",
    "    for i in range(len(data_array)):\n",
    "        res += gauss((x-data_array[i])/bandwidth)\n",
    "    res /= (N*bandwidth)\n",
    "    return res\n",
    "def lhkdekde(x,data_array,bandwidth):\n",
    "    return(np.log(get_kde(x,data_array=data_array,bandwidth=bandwidth))*get_kde(x,data_array=data_array,bandwidth=bandwidth))\n",
    "def get_H(x,data_array,bandwidth):\n",
    "    # éè¦è®¡ç®ä¸ä¸ä¸é\n",
    "    low=min(data_array)\n",
    "    std=np.std(data_array)\n",
    "    low=low-2*std\n",
    "    from scipy import integrate\n",
    "    def f(x):\n",
    "        return(lhkdekde(x,data_array=data_array,bandwidth=bandwidth))\n",
    "    return integrate.quad(f,a=low,b=x)\n",
    "def get_entropy(actions):\n",
    "    entropys=[]\n",
    "    for action in actions:\n",
    "        entropy=-action.dot(log(action))\n",
    "        entropys.append(entropy)\n",
    "    entropy_mean=np.mean(entropys)\n",
    "    return entropy_mean\n",
    "def get_ENB(actions):\n",
    "    ENBs=[]\n",
    "    for action in actions:\n",
    "        w=action[1:]\n",
    "        w_f=np.array(np.linalg.inv(eigVects).dot(w))\n",
    "        w_f=w_f[0]\n",
    "        w_f2=w_f*w_f\n",
    "        eigVals2=eigVals*eigVals\n",
    "        t=eigVals2*w_f2\n",
    "        pw=t/np.sum(t)\n",
    "        ENB=np.exp(-np.sum(pw*np.log(pw)))\n",
    "        ENBs.append(ENB)\n",
    "    return np.mean(ENBs)\n",
    "new_test=new_test[[\"date\",\"tic\",\"target\"]]\n",
    "new_test = new_test.sort_values([\"date\", \"tic\"], ignore_index=True)\n",
    "new_dataframe=pd.DataFrame()\n",
    "for date in new_test.date.unique():\n",
    "    a=new_test[new_test.date==date]\n",
    "    b=pd.DataFrame(data=[list(a.target)],columns=list(a.tic),index=a.date.unique())\n",
    "    new_dataframe=new_dataframe.append(b)\n",
    "covMat=np.cov(new_dataframe,rowvar=0)\n",
    "eigVals,eigVects=np.linalg.eig(np.mat(covMat))\n",
    "model_path=os.path.join(data_dir,model_list[loc])\n",
    "seed_file_list=os.listdir(model_path)\n",
    "entropy_means=[]\n",
    "ENB_mean=[]\n",
    "for seed in seed_file_list:\n",
    "    result=pd.read_csv( os.path.join(data_dir,model_list[loc],seed,\"result.csv\"),index_col=0)\n",
    "    num_epoch=list(result[\"eval_sr\"]).index(max(result[\"eval_sr\"]))\n",
    "    actions=np.load(os.path.join(data_dir,model_list[loc],seed,\"test_daily_action_{}.npy\".format(num_epoch)))\n",
    "    new_action=[]\n",
    "    for action in actions:\n",
    "        action=softmax_normalization(action).squeeze()\n",
    "        new_action.append(action)\n",
    "    actions=new_action\n",
    "    entropy_means.append(get_entropy(actions))\n",
    "    ENB_mean.append(get_ENB(actions))\n",
    "\n",
    "print(\"entropy mean\",np.mean(entropy_means))\n",
    "print(\"entropy std\",np.std(entropy_means))\n",
    "\n",
    "print(\"ENB mean\",np.mean(ENB_mean))\n",
    "print(\"ENB std\",np.std(ENB_mean))\n",
    "np.save(\"/home/sunshuo/qml/RL_Mix/diversity/sz50/Entropy.npy\",entropy_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best model is  en_3_batch_256_plr_0.0007_qlr_0.0007_layer_128_2_buffer_10000_discount_0.99_tem_20_bm_0.5_uncertain_0.5\n",
      "the seed_51234 's best train result in the valid is 7\n",
      "the seed_45123 's best train result in the valid is 4\n",
      "the seed_34512 's best train result in the valid is 2\n",
      "the seed_23451 's best train result in the valid is 2\n",
      "the seed_12345 's best train result in the valid is 8\n",
      "the best sr result is  0.42422131268289043\n",
      "the best tr result is  13.764689920784907 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>daily_return</th>\n",
       "      <th>reward</th>\n",
       "      <th>Cumlative Return</th>\n",
       "      <th>result1</th>\n",
       "      <th>result2</th>\n",
       "      <th>result3</th>\n",
       "      <th>result4</th>\n",
       "      <th>result5</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.008170</td>\n",
       "      <td>1.008170</td>\n",
       "      <td>1.008170</td>\n",
       "      <td>1.009285</td>\n",
       "      <td>1.008802</td>\n",
       "      <td>1.007459</td>\n",
       "      <td>1.008806</td>\n",
       "      <td>1.006498</td>\n",
       "      <td>0.001034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.003065</td>\n",
       "      <td>0.996935</td>\n",
       "      <td>1.005080</td>\n",
       "      <td>1.006363</td>\n",
       "      <td>1.005336</td>\n",
       "      <td>1.003743</td>\n",
       "      <td>1.005630</td>\n",
       "      <td>1.004325</td>\n",
       "      <td>0.000935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001696</td>\n",
       "      <td>1.001696</td>\n",
       "      <td>1.006784</td>\n",
       "      <td>1.008399</td>\n",
       "      <td>1.007343</td>\n",
       "      <td>1.005688</td>\n",
       "      <td>1.007897</td>\n",
       "      <td>1.004593</td>\n",
       "      <td>0.001426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.002524</td>\n",
       "      <td>0.997476</td>\n",
       "      <td>1.004243</td>\n",
       "      <td>1.006024</td>\n",
       "      <td>1.005024</td>\n",
       "      <td>1.002651</td>\n",
       "      <td>1.005779</td>\n",
       "      <td>1.001740</td>\n",
       "      <td>0.001729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>-0.005581</td>\n",
       "      <td>0.994419</td>\n",
       "      <td>1.109480</td>\n",
       "      <td>1.117055</td>\n",
       "      <td>1.111268</td>\n",
       "      <td>1.115483</td>\n",
       "      <td>1.060091</td>\n",
       "      <td>1.143487</td>\n",
       "      <td>0.027175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>0.006528</td>\n",
       "      <td>1.006528</td>\n",
       "      <td>1.116722</td>\n",
       "      <td>1.124332</td>\n",
       "      <td>1.119755</td>\n",
       "      <td>1.122888</td>\n",
       "      <td>1.066726</td>\n",
       "      <td>1.149878</td>\n",
       "      <td>0.027218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>0.005311</td>\n",
       "      <td>1.005311</td>\n",
       "      <td>1.122654</td>\n",
       "      <td>1.129846</td>\n",
       "      <td>1.126764</td>\n",
       "      <td>1.129186</td>\n",
       "      <td>1.071465</td>\n",
       "      <td>1.156021</td>\n",
       "      <td>0.027731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>0.007817</td>\n",
       "      <td>1.007817</td>\n",
       "      <td>1.131429</td>\n",
       "      <td>1.139113</td>\n",
       "      <td>1.136257</td>\n",
       "      <td>1.138279</td>\n",
       "      <td>1.079605</td>\n",
       "      <td>1.163890</td>\n",
       "      <td>0.027816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>0.005493</td>\n",
       "      <td>1.005493</td>\n",
       "      <td>1.137644</td>\n",
       "      <td>1.144730</td>\n",
       "      <td>1.141650</td>\n",
       "      <td>1.145997</td>\n",
       "      <td>1.085330</td>\n",
       "      <td>1.170528</td>\n",
       "      <td>0.028121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows Ã 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     daily_return    reward  Cumlative Return   result1   result2   result3  \\\n",
       "0        0.000000  1.000000          1.000000  1.000000  1.000000  1.000000   \n",
       "1        0.008170  1.008170          1.008170  1.009285  1.008802  1.007459   \n",
       "2       -0.003065  0.996935          1.005080  1.006363  1.005336  1.003743   \n",
       "3        0.001696  1.001696          1.006784  1.008399  1.007343  1.005688   \n",
       "4       -0.002524  0.997476          1.004243  1.006024  1.005024  1.002651   \n",
       "..            ...       ...               ...       ...       ...       ...   \n",
       "795     -0.005581  0.994419          1.109480  1.117055  1.111268  1.115483   \n",
       "796      0.006528  1.006528          1.116722  1.124332  1.119755  1.122888   \n",
       "797      0.005311  1.005311          1.122654  1.129846  1.126764  1.129186   \n",
       "798      0.007817  1.007817          1.131429  1.139113  1.136257  1.138279   \n",
       "799      0.005493  1.005493          1.137644  1.144730  1.141650  1.145997   \n",
       "\n",
       "      result4   result5       std  \n",
       "0    1.000000  1.000000  0.000000  \n",
       "1    1.008806  1.006498  0.001034  \n",
       "2    1.005630  1.004325  0.000935  \n",
       "3    1.007897  1.004593  0.001426  \n",
       "4    1.005779  1.001740  0.001729  \n",
       "..        ...       ...       ...  \n",
       "795  1.060091  1.143487  0.027175  \n",
       "796  1.066726  1.149878  0.027218  \n",
       "797  1.071465  1.156021  0.027731  \n",
       "798  1.079605  1.163890  0.027816  \n",
       "799  1.085330  1.170528  0.028121  \n",
       "\n",
       "[800 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sr_average=[]# this is the list to store every models' average result \n",
    "for i in range(len(model_list)):\n",
    "    result_sr=[]# this is the list to store single models result  \n",
    "    model_path=os.path.join(data_dir,model_list[i])\n",
    "    seed_file_list=os.listdir(model_path)\n",
    "    for seed in seed_file_list:\n",
    "        result=pd.read_csv( os.path.join(data_dir,model_list[i],seed,\"result.csv\"),index_col=0)\n",
    "        num_epoch=list(result[\"eval_sr\"]).index(max(result[\"eval_sr\"])) # find the num_epoch in that random seed to stop and see\n",
    "        seed_best_result=result.iloc[num_epoch][\"test_tr\"]\n",
    "        result_sr.append(seed_best_result)\n",
    "    average_result=np.average(result_sr)\n",
    "    test_sr_average.append(average_result)\n",
    "    # print(test_sr_average,model_list[i] )\n",
    "loc=test_sr_average.index(max(test_sr_average))\n",
    "print(\"the best model is \",model_list[loc])\n",
    "model_path=os.path.join(data_dir,model_list[loc])\n",
    "seed_file_list=os.listdir(model_path)\n",
    "best_results=[]\n",
    "best_crs=[]\n",
    "\n",
    "for seed in seed_file_list:\n",
    "    result=pd.read_csv( os.path.join(data_dir,model_list[i],seed,\"result.csv\"),index_col=0)\n",
    "    num_epoch=list(result[\"eval_sr\"]).index(max(result[\"eval_sr\"]))\n",
    "    print(\"the\",seed,\"'s best train result in the valid is\",num_epoch)\n",
    "    seed_best_result=result.iloc[num_epoch][\"test_sr\"]\n",
    "    seed_best_cr=result.iloc[num_epoch][\"test_tr\"]\n",
    "    best_results.append(seed_best_result)\n",
    "    best_crs.append(seed_best_cr)\n",
    "average_best_result=np.mean(best_results)\n",
    "average_cr=np.mean(best_crs)\n",
    "print(\"the best sr result is \",average_best_result)   \n",
    "print(\"the best tr result is \",average_cr*100,\"%\")   \n",
    "model_path=os.path.join(data_dir,model_list[loc])\n",
    "seed_file_list=os.listdir(model_path)\n",
    "results=[]\n",
    "for seed in seed_file_list:\n",
    "    result=pd.read_csv( os.path.join(data_dir,model_list[i],seed,\"result.csv\"),index_col=0)\n",
    "    num_epoch=list(result[\"eval_sr\"]).index(max(result[\"eval_sr\"]))\n",
    "    result=pd.read_csv(os.path.join(data_dir,model_list[i],seed,\"test_daily_return_{}.csv\".format(num_epoch)),index_col=0)\n",
    "    results.append(result)\n",
    "for result in results:\n",
    "    result[\"reward\"]=result.daily_return+1\n",
    "    y_list=[1]\n",
    "    for x in list(result.reward):\n",
    "        y=y_list[-1]*x\n",
    "        y_list.append(y)\n",
    "    y_list=y_list[1:]\n",
    "    result[\"Cumlative Return\"]=y_list\n",
    "result_1,result_2,result_3,result_4,result_5=results\n",
    "average_df=pd.DataFrame()\n",
    "average_df[\"daily_return\"]=(result_1[\"daily_return\"]+result_2[\"daily_return\"]+result_3[\"daily_return\"]+result_4[\"daily_return\"]+result_5[\"daily_return\"])/5\n",
    "average_df[\"reward\"]=average_df[\"daily_return\"]+1\n",
    "y_list=[1]\n",
    "for x in list(average_df.reward):\n",
    "    y=y_list[-1]*x\n",
    "    y_list.append(y)\n",
    "y_list=y_list[1:]\n",
    "average_df[\"Cumlative Return\"]=y_list\n",
    "average_df[\"result1\"]=result_1[\"Cumlative Return\"]\n",
    "average_df[\"result2\"]=result_2[\"Cumlative Return\"]\n",
    "average_df[\"result3\"]=result_3[\"Cumlative Return\"]\n",
    "average_df[\"result4\"]=result_4[\"Cumlative Return\"]\n",
    "average_df[\"result5\"]=result_5[\"Cumlative Return\"]\n",
    "average_df[\"std\"]=np.std([average_df.result1,average_df.result2,average_df.result3,average_df.result4,average_df.result5],axis=0)\n",
    "average_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sr_average:0.7563256171452735,sr_std:0.12137778908342486\n",
      "cr_average:0.5212244753607925,cr_std:0.06190739537013845\n",
      "sor_average:0.8349855284715122,sor_std:0.13545166211405132\n",
      "mdd_average:0.2817396289250719,mdd_std:0.019066738343010252\n",
      "vol_average:0.006897265715318709,vol_std:0.00021098673690743555\n"
     ]
    }
   ],
   "source": [
    "def evaualte(df):\n",
    "    daily_return=df[\"daily_return\"]\n",
    "    neg_ret_lst=df[df[\"daily_return\"]<0][\"daily_return\"]\n",
    "    sharpe_ratio=np.mean(daily_return)/np.std(daily_return)*(len(df)**0.5)\n",
    "    vol=np.std(daily_return)\n",
    "    mdd=max((max(df[\"Cumlative Return\"])-df[\"Cumlative Return\"])/max(df[\"Cumlative Return\"]))\n",
    "    cr=np.sum(daily_return)/mdd\n",
    "    sor=np.sum(daily_return)/np.std(neg_ret_lst)/np.sqrt(len(daily_return))\n",
    "    return sharpe_ratio,vol,mdd,cr,sor\n",
    "def all(results):\n",
    "    sharpe_ratios=[]\n",
    "    vols=[]\n",
    "    mdds=[]\n",
    "    crs=[]\n",
    "    sors=[]\n",
    "    for result in results:\n",
    "        sr,vo,mdd,cr,sor=evaualte(result)\n",
    "        sharpe_ratios.append(sr)\n",
    "        vols.append(vo)\n",
    "        mdds.append(mdd)\n",
    "        crs.append(cr)\n",
    "        sors.append(sor)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    print(\"sr_average:{},sr_std:{}\".format(np.mean(sharpe_ratios),np.std(sharpe_ratios)))\n",
    "    print(\"cr_average:{},cr_std:{}\".format(np.mean(crs),np.std(crs)))\n",
    "    print(\"sor_average:{},sor_std:{}\".format(np.mean(sors),np.std(sors) ))\n",
    "    print(\"mdd_average:{},mdd_std:{}\".format(np.mean(mdds),np.std(mdds) ))\n",
    "    print(\"vol_average:{},vol_std:{}\".format(np.mean(vols),np.std(vols) ))\n",
    "all(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './sz50/en_3_batch_256_plr_0.0007_qlr_0.0007_layer_128_2_buffer_10000_discount_0.99_tem_20_bm_0.5_uncertain_0.9'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2920969/2011533234.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mresult_sr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m# this is the list to store single models result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mseed_file_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseed_file_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"result.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './sz50/en_3_batch_256_plr_0.0007_qlr_0.0007_layer_128_2_buffer_10000_discount_0.99_tem_20_bm_0.5_uncertain_0.9'"
     ]
    }
   ],
   "source": [
    "test_sr_average=[]# this is the list to store every models' average result \n",
    "model_list=['en_3_batch_256_plr_0.0007_qlr_0.0007_layer_128_2_buffer_10000_discount_0.99_tem_20_bm_0.5_uncertain_0.9',\n",
    " 'en_3_batch_256_plr_0.0007_qlr_0.0007_layer_128_2_buffer_10000_discount_0.99_tem_20_bm_0.5_uncertain_0.7','en_3_batch_256_plr_0.0007_qlr_0.0007_layer_128_2_buffer_10000_discount_0.99_tem_20_bm_0.5_uncertain_0.3',\n",
    "  'en_3_batch_256_plr_0.0007_qlr_0.0007_layer_128_2_buffer_10000_discount_0.99_tem_20_bm_0.5_uncertain_0.1','en_3_batch_256_plr_0.0007_qlr_0.0007_layer_128_2_buffer_10000_discount_0.99_tem_20']\n",
    "for i in range(len(model_list)):\n",
    "    result_sr=[]# this is the list to store single models result  \n",
    "    model_path=os.path.join(data_dir,model_list[i])\n",
    "    seed_file_list=os.listdir(model_path)\n",
    "    for seed in seed_file_list:\n",
    "        result=pd.read_csv( os.path.join(data_dir,model_list[i],seed,\"result.csv\"),index_col=0)\n",
    "        num_epoch=list(result[\"eval_sr\"]).index(max(result[\"eval_sr\"])) # find the num_epoch in that random seed to stop and see\n",
    "        seed_best_result=result.iloc[num_epoch][\"test_sr\"]\n",
    "        result_sr.append(seed_best_result)\n",
    "    average_result=np.average(result_sr)\n",
    "    test_sr_average.append(average_result)\n",
    "    # print(test_sr_average,model_list[i] )\n",
    "    print(model_list[i])\n",
    "    print(average_result)\n",
    "loc=test_sr_average.index(max(test_sr_average))\n",
    "print(max(test_sr_average))\n",
    "print(\"the best model is \",model_list[loc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list=os.listdir(\"/home/sunshuo/qml/RL_Mix/PM/data/portfolio_management/sunrise-pm/crypto\")\n",
    "# the all_file_list are the models we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best model is  en_3_batch_256_plr_0.0007_qlr_0.0007_layer_128_2_buffer_1000_discount_0.99_tem_20_bm_0.5_uncertain_0.9\n",
      "the seed_51234 's best train result in the valid is 6\n",
      "the seed_45123 's best train result in the valid is 9\n",
      "the seed_34512 's best train result in the valid is 6\n",
      "the seed_23451 's best train result in the valid is 5\n",
      "the seed_12345 's best train result in the valid is 2\n",
      "the best sr result is  2.110420553303385\n",
      "the best tr result is  293.33554422749046 %\n"
     ]
    }
   ],
   "source": [
    "test_sr_average=[]# this is the list to store every models' average result \n",
    "for i in range(len(model_list)):\n",
    "    results=[]\n",
    "    result_sr=[]# this is the list to store single models result  \n",
    "    model_path=os.path.join(data_dir,model_list[i])\n",
    "    seed_file_list=os.listdir(model_path)\n",
    "    for seed in seed_file_list:\n",
    "        result=pd.read_csv( os.path.join(data_dir,model_list[i],seed,\"result.csv\"),index_col=0)\n",
    "        num_epoch=list(result[\"eval_sr\"]).index(max(result[\"eval_sr\"])) # find the num_epoch in that random seed to stop and see\n",
    "        result=pd.read_csv(os.path.join(data_dir,model_list[i],seed,\"test_daily_return_{}.csv\".format(num_epoch)),index_col=0)\n",
    "        results.append(result)\n",
    "    for result in results:\n",
    "        # print(result)\n",
    "        result[\"reward\"]=result.daily_return+1\n",
    "        # print(result)\n",
    "        y_list=[1]\n",
    "        for x in list(result.reward):\n",
    "            y=y_list[-1]*x\n",
    "            y_list.append(y)\n",
    "        y_list=y_list[1:]\n",
    "        result[\"Cumlative Return\"]=y_list\n",
    "    result_1,result_2,result_3,result_4,result_5=results\n",
    "    average_df=pd.DataFrame()\n",
    "    average_df[\"daily_return\"]=(result_1[\"daily_return\"]+result_2[\"daily_return\"]+result_3[\"daily_return\"]+result_4[\"daily_return\"]+result_5[\"daily_return\"])/5\n",
    "    average_df[\"reward\"]=average_df[\"daily_return\"]+1\n",
    "    y_list=[1]\n",
    "    for x in list(average_df.reward):\n",
    "        y=y_list[-1]*x\n",
    "        y_list.append(y)\n",
    "    y_list=y_list[1:]\n",
    "    average_df[\"Cumlative Return\"]=y_list\n",
    "    average_result=y_list[-1]\n",
    "    test_sr_average.append(average_result)\n",
    "    # print(test_sr_average,model_list[i] )\n",
    "\n",
    "loc=test_sr_average.index(max(test_sr_average))\n",
    "print(\"the best model is \",model_list[loc])\n",
    "model_path=os.path.join(data_dir,model_list[loc])\n",
    "seed_file_list=os.listdir(model_path)\n",
    "best_results=[]\n",
    "best_crs=[]\n",
    "\n",
    "for seed in seed_file_list:\n",
    "    result=pd.read_csv( os.path.join(data_dir,model_list[loc],seed,\"result.csv\"),index_col=0)\n",
    "    num_epoch=list(result[\"eval_sr\"]).index(max(result[\"eval_sr\"]))\n",
    "    print(\"the\",seed,\"'s best train result in the valid is\",num_epoch)\n",
    "    seed_best_result=result.iloc[num_epoch][\"test_sr\"]\n",
    "    seed_best_cr=result.iloc[num_epoch][\"test_tr\"]\n",
    "    best_results.append(seed_best_result)\n",
    "    best_crs.append(seed_best_cr)\n",
    "average_best_result=np.mean(best_results)\n",
    "average_cr=np.mean(best_crs)\n",
    "print(\"the best sr result is \",average_best_result)   \n",
    "print(\"the best tr result is \",average_cr*100,\"%\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>daily_return</th>\n",
       "      <th>reward</th>\n",
       "      <th>Cumlative Return</th>\n",
       "      <th>result1</th>\n",
       "      <th>result2</th>\n",
       "      <th>result3</th>\n",
       "      <th>result4</th>\n",
       "      <th>result5</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.148552</td>\n",
       "      <td>1.148552</td>\n",
       "      <td>1.148552</td>\n",
       "      <td>1.162694</td>\n",
       "      <td>1.154111</td>\n",
       "      <td>1.150226</td>\n",
       "      <td>1.104844</td>\n",
       "      <td>1.170887</td>\n",
       "      <td>0.022991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.042918</td>\n",
       "      <td>1.042918</td>\n",
       "      <td>1.197846</td>\n",
       "      <td>1.198063</td>\n",
       "      <td>1.212618</td>\n",
       "      <td>1.187775</td>\n",
       "      <td>1.172540</td>\n",
       "      <td>1.217205</td>\n",
       "      <td>0.016338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.020475</td>\n",
       "      <td>1.020475</td>\n",
       "      <td>1.222372</td>\n",
       "      <td>1.208272</td>\n",
       "      <td>1.243575</td>\n",
       "      <td>1.201954</td>\n",
       "      <td>1.223155</td>\n",
       "      <td>1.233296</td>\n",
       "      <td>0.015393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.032605</td>\n",
       "      <td>1.032605</td>\n",
       "      <td>1.262227</td>\n",
       "      <td>1.237562</td>\n",
       "      <td>1.276436</td>\n",
       "      <td>1.250622</td>\n",
       "      <td>1.271870</td>\n",
       "      <td>1.272811</td>\n",
       "      <td>0.015161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>-0.039723</td>\n",
       "      <td>0.960277</td>\n",
       "      <td>4.183953</td>\n",
       "      <td>4.562124</td>\n",
       "      <td>5.830302</td>\n",
       "      <td>2.730863</td>\n",
       "      <td>3.787989</td>\n",
       "      <td>4.255202</td>\n",
       "      <td>1.011399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0.003584</td>\n",
       "      <td>1.003584</td>\n",
       "      <td>4.198948</td>\n",
       "      <td>4.590343</td>\n",
       "      <td>5.834572</td>\n",
       "      <td>2.737280</td>\n",
       "      <td>3.801880</td>\n",
       "      <td>4.276411</td>\n",
       "      <td>1.011584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0.013805</td>\n",
       "      <td>1.013805</td>\n",
       "      <td>4.256915</td>\n",
       "      <td>4.654111</td>\n",
       "      <td>5.901264</td>\n",
       "      <td>2.772559</td>\n",
       "      <td>3.869690</td>\n",
       "      <td>4.331920</td>\n",
       "      <td>1.020640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.016770</td>\n",
       "      <td>1.016770</td>\n",
       "      <td>4.328304</td>\n",
       "      <td>4.721665</td>\n",
       "      <td>5.982814</td>\n",
       "      <td>2.821187</td>\n",
       "      <td>3.961742</td>\n",
       "      <td>4.393386</td>\n",
       "      <td>1.028658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>-0.036951</td>\n",
       "      <td>0.963049</td>\n",
       "      <td>4.168369</td>\n",
       "      <td>4.549145</td>\n",
       "      <td>5.758407</td>\n",
       "      <td>2.720728</td>\n",
       "      <td>3.797989</td>\n",
       "      <td>4.245039</td>\n",
       "      <td>0.990091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>186 rows Ã 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     daily_return    reward  Cumlative Return   result1   result2   result3  \\\n",
       "0        0.000000  1.000000          1.000000  1.000000  1.000000  1.000000   \n",
       "1        0.148552  1.148552          1.148552  1.162694  1.154111  1.150226   \n",
       "2        0.042918  1.042918          1.197846  1.198063  1.212618  1.187775   \n",
       "3        0.020475  1.020475          1.222372  1.208272  1.243575  1.201954   \n",
       "4        0.032605  1.032605          1.262227  1.237562  1.276436  1.250622   \n",
       "..            ...       ...               ...       ...       ...       ...   \n",
       "181     -0.039723  0.960277          4.183953  4.562124  5.830302  2.730863   \n",
       "182      0.003584  1.003584          4.198948  4.590343  5.834572  2.737280   \n",
       "183      0.013805  1.013805          4.256915  4.654111  5.901264  2.772559   \n",
       "184      0.016770  1.016770          4.328304  4.721665  5.982814  2.821187   \n",
       "185     -0.036951  0.963049          4.168369  4.549145  5.758407  2.720728   \n",
       "\n",
       "      result4   result5       std  \n",
       "0    1.000000  1.000000  0.000000  \n",
       "1    1.104844  1.170887  0.022991  \n",
       "2    1.172540  1.217205  0.016338  \n",
       "3    1.223155  1.233296  0.015393  \n",
       "4    1.271870  1.272811  0.015161  \n",
       "..        ...       ...       ...  \n",
       "181  3.787989  4.255202  1.011399  \n",
       "182  3.801880  4.276411  1.011584  \n",
       "183  3.869690  4.331920  1.020640  \n",
       "184  3.961742  4.393386  1.028658  \n",
       "185  3.797989  4.245039  0.990091  \n",
       "\n",
       "[186 rows x 9 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path=os.path.join(data_dir,model_list[loc])\n",
    "seed_file_list=os.listdir(model_path)\n",
    "results=[]\n",
    "for seed in seed_file_list:\n",
    "    result=pd.read_csv( os.path.join(data_dir,model_list[loc],seed,\"result.csv\"),index_col=0)\n",
    "    num_epoch=list(result[\"eval_sr\"]).index(max(result[\"eval_sr\"]))\n",
    "    result=pd.read_csv(os.path.join(data_dir,model_list[i],seed,\"test_daily_return_{}.csv\".format(num_epoch)),index_col=0)\n",
    "    results.append(result)\n",
    "for result in results:\n",
    "    result[\"reward\"]=result.daily_return+1\n",
    "    y_list=[1]\n",
    "    for x in list(result.reward):\n",
    "        y=y_list[-1]*x\n",
    "        y_list.append(y)\n",
    "    y_list=y_list[1:]\n",
    "    result[\"Cumlative Return\"]=y_list\n",
    "result_1,result_2,result_3,result_4,result_5=results\n",
    "average_df=pd.DataFrame()\n",
    "average_df[\"daily_return\"]=(result_1[\"daily_return\"]+result_2[\"daily_return\"]+result_3[\"daily_return\"]+result_4[\"daily_return\"]+result_5[\"daily_return\"])/5\n",
    "average_df[\"reward\"]=average_df[\"daily_return\"]+1\n",
    "y_list=[1]\n",
    "for x in list(average_df.reward):\n",
    "    y=y_list[-1]*x\n",
    "    y_list.append(y)\n",
    "y_list=y_list[1:]\n",
    "average_df[\"Cumlative Return\"]=y_list\n",
    "average_df[\"result1\"]=result_1[\"Cumlative Return\"]\n",
    "average_df[\"result2\"]=result_2[\"Cumlative Return\"]\n",
    "average_df[\"result3\"]=result_3[\"Cumlative Return\"]\n",
    "average_df[\"result4\"]=result_4[\"Cumlative Return\"]\n",
    "average_df[\"result5\"]=result_5[\"Cumlative Return\"]\n",
    "average_df[\"std\"]=np.std([average_df.result1,average_df.result2,average_df.result3,average_df.result4,average_df.result5],axis=0)\n",
    "average_df"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "05c7ecc4e09568b16608f47442989c4eaf2bc45e71bc9e50b36593a814f6f101"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('FinRL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
